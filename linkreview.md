# Industry


| компания | статья/доклад | комментарий | loss | граф | модель |
|-|-|-|-|-|-|
| Twitter |  (2022) TwHIN. Embedding the Twitter Heterogeneous Information Network for Personalized Recommendation ((https://arxiv.org/pdf/2202.05387.pdf)) | Твиттер их использует как самостоятельный кандген в рекламе и рекомендации твиттов, а также переиспользует их в своем тяжелом ранжировании. | 1. BCE(link-prediction); негативы семплируются из двух источников: равномерное семплирование (~1000 uniform negative sampling) и семплироввание, пропорциональное популярности вершины (~1000 in-batch негативов). 2. В ((https://github.com/twitter/the-algorithm the-algorithm)) негативная часть лосса перевзвешивается, чтобы позитивная и негативная части вносили одинаковый вклад; используются только in-batch негативы. | 1. Сильно гетерогенный граф; Типы вершин: твит, пользователь, рекламный баннер, рекламодатель, видео, приложение(?). Типы ребер: (юзер, автор, твит), (юзер, follows, юзер), (user, favourites, твит), (рекламодатель, promotes, ad), (юзер, replies, твит), (юзер, retweets, твит), (юзер, click, ad).  2. Directed multigraph.  3. вершин = 1e8, ребер = 1e10 | 1. Трансдуктивная модель. 2. обучаются только айдишники; близость через $(\theta_u + \theta_t)^T \theta_i$. |
| Pinterest | (2018) Graph Convolutional Neural Networks for Web-Scale Recommender Systems ((https://arxiv.org/pdf/1806.01973.pdf)) | Первая версия графовых векторов Pinteres - "PinSage". | 1. В качестве положительной пары выступает не ребро в графе, а пара вида (пин, на котором пользователь находится сейчас; пин, на который пользователь перешел с этого пина). Обозначается (q, e). 2. Max-margin loss; для батча отбирался один набор из 500 случайных негативов и шарился между элементами батча; дополнительно использовались hard негативы, которые отбирались с позиций 2000-5000 по Personalized PageRank scores относите вершины q. | Двудольный граф board-pin; ребро в графе между board и pin означает, что этот pin был добавлен в этот board; вершин = 1e9, ребер = 1e10. | 1. Индуктивная модель.  2. используется контент: картиночный эмбед, текстовый эмбед. 3. GraphSAGE-like; соседи отбираются с помощью случайных блужданий; 2-х уровневая свертка. |
| Pinterest | ((https://www.cs.emory.edu/~jyang71/files/multisage.pdf (2020) MultiSage: Empowering GCN with Contextualized Multi-Embeddings on Web-Scale Multipartite Networks)) | Вторая версия графовых векторов Pinterest - "MultiSage". Ключевое отличие от предыдущей - это добавление механизма внимания при агрегации и переход к одноуровневой свертке.| Аналогично "PinSage". |Двудольный граф board-pin; ребро в графе между board и pin означает, что этот pin был добавлен в этот board; вершин = 1e9, ребер = 1e10.| 1. Индуктивная модель.  2. При агрегации соседей используется context masking + multi-head attention. 3. От 2-х уровней свертки перешли к 1. |
| Pinterest | (2022) MultiBiSage: A Web-Scale Recommendation System Using Multiple Bipartite Graphs at Pinterest ((https://arxiv.org/pdf/2205.10666.pdf)) | Третья версия графовых векторов Pinterest - "MultiBiSage". Ключевое отличие от предыдущей - рассматривается граф с большим числом различных типов вершин и ребер.  | 1. Позитивы такие же, как в предыдущих подходах.  2. Лосс = сумма двух sampled softmax, один по in-batch негативам, другой по случайным негативам; log-Q| 1. Рассматривается несколько двудольных графов: pin-board, user-product, user-ad, searchQuery-pin. 2. вершин = 2e9, ребер = 1e10.  | 1. Индуктивная модель. 2. Для агрегации соседей перешли к трансформеру. 3. Сначала агрегируют по каждому двудольному графу отдельно, а потом с помощью отдельного трансформера получают итоговый векторов. |
| Google | (2020) Grale: Designing Networks for Graph Learning ((https://arxiv.org/pdf/2007.12002.pdf)) | Решается задача semi-supervised learning. И предлагается алгоритм, который по заданным мульти-модальным объектам строит ребра между ними. Не совсем то, что хотим. | - | - | - |
| Google | (2023) HUGE. Huge Unsupervised Graph Embeddings with TPUs ((https://arxiv.org/pdf/2307.14490.pdf)) | тпу у нас нет:) | - | - | - |
| Amazon | (2021) Graph-based Multilingual Product Retrieval in E-Commerce Search ((https://arxiv.org/pdf/2105.02978.pdf)) | Решается задача ритривела для поисковых запросов в e-commerce. Графовое представление здесь используется как дополнительная фича кандидата в двухбашенной модели. Учет графовых соседей помогает им лучше работать на других языках. | отдельного лосса для графов нет, т.к. обучается end-to-end со всей моделью | searchQuery-item граф | 1. Индуктивная модель 2. Запросы (отобранные соседи) пропускаются через берты, а дальше представления с CLS токенов усредняются, все канкатенируется с продуктов и проходит через MLP. |
| Amazon | (2023) G-STO: Sequential Main Shopping Intention Detection via Graph-Regularized Stochastic Transformer ((https://arxiv.org/pdf/2306.14314.pdf)) | здесь что-то не то | - | - | - |
| Amazon | (2023) Multi-Task Knowledge Enhancement for Zero-Shot and Multi-Domain Recommendation in an AI Assistant Application ((https://arxiv.org/pdf/2306.06302.pdf)) | Решается проблема холодных пользователей за счет переиспользования знаний из соседних доменов. | Сумма рекомендательного (margin ranking loss) и лосса от графа знаний. | 1. Рассматривается 3 домена: музыка, видео, книги. Строится 4 графа: кросс-доменный и по доменам.  2. Вершин = 1e7, ребер = 1e8. | 1. Трансдуктивная модель по айтемам.  2. Представление пользователя получается индуктивно с помощью attention-based gnn в каждом из 4-х графов. Для айтемов есть 2 обучаемые матрица - одна для KG-лосса, а другая для рекомендательного лосса. 3. Для рекомендательного лосса представлением пользователя = конкатенация кросс-доменного и конкретного домена. Представление айтема = конкатенация двух обучаемых векторов: рекомендательного и KG. |
| Spotify | (2020) "Podcast Recommendations and Search Query using GNNs at Spotify. Graph Learning Workshop 2022" ((https://youtu.be/79MRwEB5AhA)) | Доклад от спотифая на одной из конференций. В нем они делятся инсайдами про то, как у них выглядят графовые сетки. Они решают задачу создания независимых от конкретной рекомендательной поверхности всеобъемлющих векторов пользователя и контента. Эти вектора они переиспользуют для кандгена и как фича в ранжировании. Также они говорят, что графовые вектора улучшают им рекомендательные метрики для long-tail. | link-prediction | Сильно гетерогенный граф: поисковые запросы, юзеры, подкасты, странички википедии (предобученные). | 1. Индуктивная модель, GraphSage-like. 2. Причем на сколько понял все вектора заморожены (включая текстовые), а вектор юзера получается на основе его соседей.|
| Spotify | ((https://www.rishabhmehrotra.com/papers/ismir2021-multi-task-representations.pdf (2021) Multi-Task Learning Of Graph-Based Inductive Representations Of Music Content)) | Предлагают усложнение link-prediction задачи за счет добавление дополнительных таргетов помимо предсказания ребра. | Multi-Task Loss(взвешенная сумма трех): BCE - пара вершин принадлежит одному плейлисту (link-prediction), BCE - пара вершин имеет один и тот же жанр, регрессия - близость с точки зрения контента. | 1. Взвешенный item-item граф, где ребро проводится, если два трека лежат в одном плейлисте. Вес ребра = количество плейлистов, в которые эта пара треков входит. 2. вершин = 16K, ребер = 5.2M, остаются только ребра с весом >= 10. | 1. Идуктивная модель. 2. GraphSage-like архитектура.  3. Используется контент трека: фичи про жанр, громкость, аудио и т.д. |
| Alibaba (Taobao) | (2023) Graph Contrastive Learning with Multi-Objective for Personalized Product Retrieval in Taobao Search ((https://arxiv.org/pdf/2307.04322.pdf)) | personalized e-commerce search retrieval; графовые соседи здесь используются как дополнительная фича при построении представления кандидата. У них есть аблейшан и графовая часть дает подозрительно мало профита в оффлайне. | contrastive-loss между представлением, полученным из исходного графа и аугментированного. | item-item граф, где ребро между айтемами проводится в случае, если они одной категории и пользователь за последние S дней на них кликнул. | Соседи семплируются пропорционально кратности ребер. В качестве агрегации attention. |
| Alibaba Group | (2022) Multi-level Contrastive Learning Framework for Sequential Recommendation ((https://arxiv.org/pdf/2208.13007.pdf)) | идея: улучшение последовательных рекомендаций за счет contrastive learning-га на обучении между представлением пользователя из sequential recommender-а и графового представления (типа сближаем текущие интересы с общими), на инференсе граф откидываем. | Сумма трех contrastive лоссов: между user из графа user-item и user из графа user-user, между item из графа item-item и графа usre-item и между выходом sequential recommender-а и представления  user из user-item. | 3 графа: user-user, item-item, user-item. | Трансдуктивная модель. Конкретный GraphEncoder не указан. |
| Alibaba Group | (2019) AliGraph: A Comprehensive Graph Neural Network Platform ((https://arxiv.org/pdf/1902.08730.pdf)) | фреймворк (open-source) | | | |
| Facebook AI Research | (2019) PyTorch-BigGraph: A Large-Scale Graph Embedding System ((https://arxiv.org/pdf/2202.05387.pdf)) | фреймворк(open-source) | TransE, ...| гетерогенный граф | трансдуктивная модель |
| Tencent |  ((https://conferences.computer.org/icde/2020/pdfs/ICDE2020-5acyuqhpJ6L9P042wmjY1p/290300b549/290300b549.pdf (2020) PSGraph: How Tencent trains extremely large-scale graphs with Spark?)) | фреймворк | - | - | трансдуктивные |
| Tencent |((https://www.semanticscholar.org/reader/deb32aacda8dc90b05eb2c1a0389b9d313be5b6e (2021) A Distributed Multi-GPU System for Large-Scale Node Embedding at Tencent)) | фремворк | - | - | трансдуктивные |
| Etsy | (2023) Unified Embedding Based Personalized Retrieval in Etsy Search ((https://arxiv.org/pdf/2306.04833.pdf)) | Решается задача ритривела для поисковых запросов в e-commerce. Графовое представление здесь используется как дополнительная фича кандидата в двухбашенной модели.  | Нет отдельного графового лосса, т.к. графовые вектора учатся вместе со всей моделью и используются как доп. фича кандидата. | Двудольный граф searchQuery-item. Про кол-во вершин/ребер не говорят, но говорят, что собрали за год поисковых логов. | 1. Семплируют запросы для кандидата, а дальше просто усредняют их вектора. 2. Используют ту же матрицу токенов, что и для поискового запроса, причем графовый вектор при обучении замораживают, т.к. иначе наблюдают переобучение. |
| KuaiShou | (2023) A Unified Model for Video Understanding and Knowledge Embedding with Heterogeneous Knowledge Graph Dataset ((https://arxiv.org/pdf/2211.10624.pdf)) | Получаем вектора short-videos и тегов к ним на основе мульти-модального контента и графа знаний. | 1. 3 стадии обучения. Сначала предобучается энкодер видео на задачу классификации тега. Потом учится энкодер тэга на задачу CLIP (энкодер видео заморожен). Далее все вместе дообучается на меньшем датасете на сумму трех лоссов: от графа знаний, от CLIP, от классификации тега. | 1. Граф состоит из 2-х частей. В первой части ребра между видео и его тегом. Во второй части ребра между некоторыми "entities", причем что за "entities" не раскрывается (см. статью). 2. Вершин, ребер 1e9. | 1. Индуктивная модель на основе контетна: текст, аудио, видео. 2. Граф знаний. |
| Meituan | (2020) HeroGRAPH: A Heterogeneous Graph Framework for Multi-Target Cross-Domain Recommendation ((https://ceur-ws.org/Vol-2715/paper6.pdf)) | Архитектруа, которую у себе Амазон использует для агрегации соседей. Ключевая фишка - умеет обрабатывать данные из разных доменов. | BPR | Кросс-доменный user-item граф. | 1. Трансдуктивная модель. 2. Предложенный в этой статье recurrent attention. |


# Academia


| университет/организация| статья/доклад | комментарий | loss | граф | модель |
|-|-|-|-|-|-|
| Damo Academy, Alibaba Group | (2021) Dynamic Sequential Graph Learning for Click-Through Rate Prediction ((https://arxiv.org/pdf/2109.12541.pdf)) | Статья про динамическое конструирование графа с учётом таймстэмпов | BCE на предсказание взаимодействия | Граф юзеров-айтемов с ребром в виде взаимодействия. Новизна графа в том, что мы сэмплируем не только позитив юзер-айтем, но и таймстэмп. Для заданного таймстэмпа мы берём соседей в графе, с которыми вершины взаимодействовали до этого таймстэмпа. | Фичи вершин задаются как конкатенация векторов их разных фичей, эмбед ребра это категориальный эмбед дискретизованной разности между timestamp сэмпла и timestamp взаимодействия. GCN с Self-Attention агрегацией.  |
| Tsinghua University, Huawei Noah’s Ark Lab | (2023) Uncertainty-aware Consistency Learning for Cold-Start Item Recommendation ((https://arxiv.org/pdf/2308.03470.pdf)) | По сути, это ещё один pseudo-labeling-аугментирующий пайплайн | $ -ln(\sigma(u_i^Td_i)) $ на задачу, InfoNCE-лосс в качестве contrastive loss между эмбедами аугментированной и оригинальной вершины   | Двудольный граф с 1 типом взаимодействий | Странная процедура pseudo-labeling для добавления рёбер - берём для вершины юзера айтемы с их скорами как дот-продакты, сортируем, а потом оставляем те айтемы, с которыми высокий косинус. Другой вид аугментации - dropout рёбер. Параллельно учим 2 модели - на оригинальном графе и на аугментированных сэмплах, и дополнительно синхронизируем веса этих 2 моделей |
| University of Illinois Chicago, Salesforce AI Research | (2023) Graph-based Alignment and Uniformity for Recommendation ((https://arxiv.org/pdf/2308.09292.pdf)) | Работа, в которой применяют GCN и считают взвешенную сумму Alignment-лоссов и Uniformity-лоссов на всех слоях   | Здесь одновременно минимизируется расстояние в позитивных пара юзер-айтем (это Alignment-лосс), но при этом есть регуляризация, раздвигающая все эмбеддинги друг от друга - это как раз Uniformity-лосс | Двудольный граф юзеров и документов, один вид ребра, если было взаимодействие между ними | Типичная N-слойная GCN |
|Stanford University | (2018) Inductive Representation Learning on Large Graphs ((https://arxiv.org/pdf/1706.02216.pdf)) | | $ -ln(\sigma(u_i^Td_i)) - Q * ln(\sigma(-u_i^Td_j)) $ | Подходит для любого графа, эксприментируют на очень разных данных  | По сути это GCN, но вместо трансдуктивного обучения таблицы с эмбедами мы работаем с вектором фичей каждой вершины на нулевом слое. Кроме того, соседей мы сэмплируем с помощью случайных блужданий. |
|The Hong Kong Polytechnic University | (2018) Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning ((https://arxiv.org/pdf/1801.07606.pdf)) | Какая-то дичь и не совсем в тему нашего исследования | Статья не про лосс | Гомогенный граф, на котором мы хотим предсказывать метку для каждой вершины. Для части вершин метки уже известны. | Какая-то куча сложной математики, на которую стоит тратить время. Добавляется 2 прикола:  1. Pseudo-labeling - после обучения размечаем самые уверенные образцы и обучаемся на них дальше 2. Специальным костылём множим метки класса между похожими вершинами в трейнсете |
|Alibaba Group, WeChat (Tencent) | (2020) Single-Layer Graph Convolutional Networks For Recommendation ((https://arxiv.org/pdf/2006.04164.pdf)) | Авторы предлагают не использовать много слоёв GCN засчёт своеобразного препроцессинга графа | BCE на дот-продакт с рандомными негативами | Вместо обычного двудольного графа мы делаем так: для каждого типа вершин изолированно строим новый граф, где расстояние между вершинами определяется "похожестью распределения их соседей". Если быть более точным, берётся два вектора P - распределения на соседей вершин, и между ними считается L2 норма. Для всех видов рёбер данное расстояние складывается.  | Трансдуктивная модель; Однослойная GCN с последующим MLP |
| University of Science and Technology of China | (2020) LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation ((https://arxiv.org/pdf/2002.02126.pdf)) | Авторы проводят эксперименты и показывают, что в модели NGCF - топовой (вроде бы) GCN на тот момент, нелинейности и многослойные операции над эмбедами вершин не нужны, если это трансдуктивная модель. Они в своей версии заменяют модель на простое усреднение соседних вершин и получают ещё более высокий результат, чем baseline. | Bayesian Personalized Ranking лосс, иначе говоря - $ ln(\sigma(u_i^Td_i - u_i^Td_j)) $ | Двудольный граф юзеров и документов | Трансдуктивная модель; GCN с простым усреднением по всем соседям в несколько слоёв. Потом эмбеды вершины со всех слоёв усредняются. |
| Universit ́e de Technologie de Compiegne, Google |((https://proceedings.neurips.cc/paper_files/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf (2013) Translating Embeddings for Modeling Multi-relational Data (TransE))) | Видимо, это статья-раритет, где придумали складывать эмбеды сущности и отношения | max-margin лосс, в качестве скора используется L1 или L2-расстояние | Граф с сущностями и разными видами отношений между ними | 1. Трансдуктивная модель. 2. обучаются только айдишники; близость через $(\theta_u + \theta_t)^T \theta_i$. 
| University of Hong Kong | (2023) LightGCL: Simple Yet Effective Graph Contrastive Learning For Recommendations ((https://arxiv.org/pdf/2302.08191.pdf)) | Метод аугментации графа для обучения. Матрица смежности подвергается неполному SVD-разложению, считается её неполноранговое приближение. Далее это приближение используют как аугментацию, т.е. подают в GCN и получают копию эмбеддингов вершин. Навешивается регуляризация на то, что аугментированные эмбеды после применения GCN должны быть близки к оригинальным. | InfoNCE-лосс - стремимся сблизить аугментированные эмбеды с оригинальными у одной вершины и отдалить у разных  | Обычный двудольный граф юзеров-айтемов | Трансдуктивная GCN в качестве базовой модели |
| BNRist | (2023) A Survey of Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions ((https://arxiv.org/pdf/2109.12843.pdf)) | просто хороший сервей | - | - | - |
| Sun Yat-sen University | (2023) Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces ((https://arxiv.org/pdf/2211.03536.pdf)) | еще один хороший сервей | - | -  | - |

