\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

% \usepackage[
% backend=biber,
% style=alphabetic,
% sorting=ynt
% ]{biblatex}



\title{GraphSASRec: модель последовательных рекомендаций на основе самовнивания, дополненная графовыми представлениями.}

\author{ 
	\textbf{Матвеев Артем} \\
    Московский государственный университет имени М. В. Ломоносова \\
	\texttt{matfu21@ya.ru} \\
}
\date{2023}

\renewcommand{\shorttitle}{}

\begin{document}
\maketitle

\begin{abstract}
	% \lipsum[1]
	Последовательные модели, решающие задачу предсказания следующего взаимодействия пользователя на 
	основе кодирования его исторических событий, являются популярным решением для построение 
	персонализированных рекомендательных систем как в индустрии, так и в академии. Преимуществами 
	таких моделей являются: учет порядка, в котором следуют исторические события и оценка долгосрочных 
	интересов пользователя. Однако подобные подходы недостаточно эксплуатируют полезный коллаборативный
	сигнал и плохо представляют объекты из длинного хвоста. Популярным решением этих проблем являются 
	методы, основанные на применении графовых нейронных сетей к двудольному графу взаимодействий 
	пользователей. В работе предлагается с другой стороны взглянуть на модель последовательных рекомендаций
	как на графовую сеть со стороны пользователя. Предлагается метод увеличения глубины этой сети,
	сохраняющий длину истории пользователя, при этом имея минимальные накладные расходы. Наблюдаемые 
	результаты демонстрируют улучшение с точки зрения метрик ранжирования и разнообразия выдачи.
\end{abstract}


\keywords{Информационный поиск \and Последовательные рекомендации \and Графовые нейронные сети}

\section{Введение}

Последовательные рекомендательные системы - это класс рекомендательных систем, которые принимают 
во внимание порядок взаимодействий пользователя и пытаются предсказать следующее его взаимодейсвие.
Учет порядка является важной состоявляющей во многих рекомендательных сценариях: если пользователь
только что купил мобильный телефон, то следующая покупка с большой долей вероятности будет 
аксессуаром к нему. Ранее такая задача решалась с помощью подходов, основанных на Марковских 
цепях \cite{mc} или рекуррентных нейронных сетях \cite{rnn1,rnn2,rnn3}. Но после появления архитектуры
трансформер \cite{transformer} и его успеха в задачах распознования естественного языка, наиболее успешными
стали модели, основанные на обработке последовательностей действий пользователя с помощью трансформера 
\cite{sasrec,bert4rec,gsasrec}. 

Другим популярным подходом в рекомендательных системах являются графовые нейронные 
сети \cite{lightgcn,lightgcl,hetergcl,herograph,slgcn}. В этом подходе информация, которая есть в рекомендательной 
системе, рассматривается с точки зрения графов. Большинство данных в любой рекомендательной системе по
существу имееют графовую структуру. Например, данные взаимодействий в рекомендательном сервисе могут быть
представлены в ввиде двудольного графа, вершины одной доли которого - пользователи, другой объекты (например, товары), а
наблюдаемый взаимодейсвия - ребра. Пусть нам дан такой граф. Ключевая идея графовых нейронный сетей заключается в
итеративной агрегации признаковых представлений соседей в графе и объединение этой агрегированной информации
с представлением вершины, для которой рассматривается соседство \cite{survey1}. Такая операция называется распространением
сообщений \cite{lightgcn,sage}. Формально ее можно записать в следующем виде:

$$
Aggregation: n^{(l)}_v = Aggregator_l(\{h^l_u, \forall u \in \mathcal{N}_v\}),
$$
$$
Update: h^{(l + 1)}_v = Updater_l(h_v^{(l)}, n_v^{(l)}),
$$

где $h_u^{(l)}$ определяется как представление вершины $u$ после $l$-ого слоя графовой сети. $Aggregator_l$ и $Update_l$
представляют собой обучаемые функции агрегации соседей и обновления представления вершины на $l$-ом слое. В качестве функции 
агрегации могут выступать как простые варианты по типу max-pooling, mean-pooling \cite{sage}, так и более сложные, например: 
importance-pooling \cite{pinsage}, агрегация на основе контекста \cite{multisage}, механизм внимания \cite{gat}, агрегация
на базе трансформера \cite{multibisage}. В качестве функции обновления могут выступать как простые архитектуры на базе несколькоих
полносвязных слоев \cite{sage, pinsage}, так и более сложные на базе трансформера \cite{multibisage}.


продолжения...




\section{Постановка задачи}

\subsection{Последовательные рекомендации}
В последовательных рекомендациях рассматривается задача предсказания следующего положительного взаимодействия 
пользователя по последовательности его исторических действий $S^u = (S_1^u, S_2^u, \dots, S_{|S^u|}^u)$.
Во время обучения, на момент времени $t$, модель предсказывает следующий объект интереса пользователя на основе его
взаимодействий, произошедших раньше момента $t$. На вход модели поступает последоватлеьность
$(S_1^u, S_2^u, \dots, S^u_{|S^u| - 1})$. Ожидаемый выход модели - следующее положительное взаимодействия $S^u_{|S^u|}$. 
В данной работе рассматривается модель SASRec \citep{sasrec}, в основе которой лежит декодер блок трансформера \cite{transformer},
на выходе которого тоже последовательность. 
Поэтому задачу можно переформулировать в эквивалентном виде, как задачу предсказания сдвинутой версии 
последовательности $(S_2^u, S_3^u, \dots, S^u_{|S^u|})$.

\subsection{Предсказание ребра}

Дан граф $G = (V, E, X)$, где $V$ - множество вершин, $E$ - множество ребер, $X \in \mathbb{R}^{|V| \times d}$
- $d$-размерные векторные представления входных вершин. Задача предсказания ребра формулируется как задача
определения существования (или появления в будущем, если граф рассматривается как динамический \cite{dyngnn}) ребра $e_{ij}$
между вершинами $i$ и $j$, где $i, j \in V$, и $e_{ij} \not \in E$.

\section{Сопутствующие работы}



\section{Метод}

\section{Эксперименты}

\section{Заключение}

\bibliographystyle{abbrv}
\bibliography{references}

\end{document}